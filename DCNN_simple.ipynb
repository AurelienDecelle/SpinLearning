{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to classify if two samples are or not in the same gauge-orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import tools as t\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear size L= 5\n",
      "Number of samples NS= 20000\n"
     ]
    }
   ],
   "source": [
    "# linear size of the system\n",
    "L=5\n",
    "Lx = L\n",
    "Ly = L\n",
    "\n",
    "# Number of sites\n",
    "N = Lx*Ly\n",
    "\n",
    "# to be commented\n",
    "connectivity,getindex=t.compute_connectivity(Lx,Ly)\n",
    "\n",
    "ncmax=int(0.5*len(connectivity))\n",
    "\n",
    "# number of samples\n",
    "NS=20000\n",
    "# periodic boundary conditions\n",
    "PBC=False\n",
    "\n",
    "print(\"Linear size L=\",L)\n",
    "print(\"Number of samples NS=\",NS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 10, 10, 2)\n",
      "(8000, 10, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "NS_GG=NS #number of samples of (gauge,gauge)\n",
    "NS_GL=NS #number of samples of (gauge,not gauge)\n",
    "NS_TOT=NS_GG + NS_GL\n",
    "\n",
    "if (PBC):\n",
    "    xdim = 2 * Lx + 1\n",
    "    ydim = 2 * Ly + 1\n",
    "else:\n",
    "    xdim = 2 * Lx \n",
    "    ydim = 2 * Ly \n",
    "    \n",
    "DataSet = np.zeros((NS_GG  + NS_GL, xdim, ydim, 2))\n",
    "Labels = np.zeros((NS_GG + NS_GL))\n",
    "\n",
    "for ns in range(NS_GG + NS_GL):\n",
    "\n",
    "    if(ns<NS_GG): # we start with the couples Gauge-gauge\n",
    "\n",
    "        #we create a random sample\n",
    "        Chess = t.createSample_2D(connectivity, Lx,Ly)\n",
    "        #we create a random orbit from it\n",
    "        Orbit = t.getOrbit_2D(Chess,connectivity, Lx,Ly)\n",
    "\n",
    "        if (PBC):\n",
    "            DataSet[ns, :, :, 0] = t.write_PBC(Chess,Lx,Ly)\n",
    "            DataSet[ns, :, :, 1] = t.write_PBC(Orbit)\n",
    "        else:\n",
    "            DataSet[ns, :, :, 0] = Chess\n",
    "            DataSet[ns, :, :, 1] = Orbit\n",
    "\n",
    "        Labels[ns]=0 # gauge-gauge\n",
    "\n",
    "    else:  # we create couples gauge-not gauge\n",
    "\n",
    "        transformation_type=np.random.randint(3)\n",
    "        #we create a random sample\n",
    "        Chess = t.createSample_2D(connectivity, Lx,Ly)\n",
    "\n",
    "        if transformation_type==0: # we invert a random number of links\n",
    "            #random number of links in between 1 and Lx*Ly (all plaquettes changed)\n",
    "            nchanges=np.random.randint(1,int(0.25*len(connectivity)))\n",
    "            q=float(nchanges/len(connectivity))\n",
    "\n",
    "            #invert q of the total couplings randomly\n",
    "            Trasf = t.getRandom_2D(Chess, connectivity, q, Lx,Ly)\n",
    "\n",
    "        if transformation_type==1: # we invert a line\n",
    "            Trasf = t.getLine_2D(Chess,connectivity, Lx,Ly)\n",
    "\n",
    "        if transformation_type==2: # invert only 1-5 random links \n",
    "            nchanges=np.random.randint(1,5)\n",
    "            q=float(nchanges/len(connectivity))\n",
    "            Trasf = t.getRandom_2D(Chess, connectivity, q, Lx,Ly)\t\n",
    "\n",
    "        # We generate a random gauge orbit of this transformation (otherwise it would be \n",
    "        # too easy for the machine )\n",
    "        Trasf = t.getOrbit_2D(Trasf,connectivity, Lx,Ly)\n",
    "\n",
    "        if(PBC):\n",
    "            DataSet[ns, :, :, 1] = t.write_PBC(Trasf,Lx,Ly)\n",
    "            DataSet[ns, :, :, 0] = t.write_PBC(Chess,Lx,Ly)\n",
    "        else:\n",
    "            DataSet[ns, :, :, 1] = Trasf   \n",
    "            DataSet[ns, :, :, 0] = Chess \n",
    "\n",
    "        Labels[ns]=1 # gauge - not gauge\n",
    "\n",
    "# we reshuffle the data to mix the two kinds\n",
    "permutation = np.random.permutation(Labels.shape[0])\n",
    "DataSet = DataSet[permutation,:,:,:]\n",
    "Labels = Labels[permutation]\n",
    "\n",
    "# We split the data set in 0.8 for the training - 0.2 for the test\n",
    "X_train,y_train,X_test,y_test = t.SplitSet(DataSet,Labels,0.8)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of the neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "from keras import regularizers, metrics\n",
    "from keras.optimizers import SGD, Adam,K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "data_dim_x = xdim\n",
    "data_dim_y = ydim\n",
    "\n",
    "Strain = X_train.shape[0]\n",
    "Stest = X_test.shape[0]\n",
    "\n",
    "Nfilter=64\n",
    "input_data = Input(shape=(data_dim_x,data_dim_y,2))\n",
    "\n",
    "# one layer vertical slab CNN\n",
    "vert_conv2D = Conv2D(Nfilter,(1,data_dim_y),activation='relu',strides=(1,1),padding='valid')(input_data)\n",
    "vert_conv2D = Flatten()(vert_conv2D)\n",
    "\n",
    "# one layer horizontal slab CNN\n",
    "horiz_conv2D = Conv2D(Nfilter,(data_dim_x,1),activation='relu',strides=(1,1),padding='valid')(input_data)\n",
    "horiz_conv2D = Flatten()(horiz_conv2D)\n",
    "\n",
    "# one layer plaquettes CNN\n",
    "plaq_conv2D = Conv2D(Nfilter,(3,3),activation='relu', padding='same',strides=(2,2))(input_data)\n",
    "plaq_conv2D = Flatten()(plaq_conv2D)\n",
    "\n",
    "# we concatenate all three results\n",
    "x = concatenate([vert_conv2D, horiz_conv2D, plaq_conv2D])\n",
    "\n",
    "# Dense layer\n",
    "x = Dense(Nfilter,activation='relu')(x)\n",
    "\n",
    "# Output\n",
    "main_output = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "DCNN = Model(input_data,main_output)\n",
    "\n",
    "#funcionan\n",
    "opt_sgd = SGD(lr=0.01,momentum=0.5,nesterov=True) \n",
    "opt_adam = Adam()\n",
    "\n",
    "#batch size\n",
    "b_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't find a simple way to do the learning and a combination of optimizers were necessary to be able to fit the machine for all the values of $L$ and $N_S$ and to avoid overfitting. The learning process ends when the accuracy on the test set goes beyond 0.995. We show a configuration that worked for all the parameters used in the paper. Sometimes the learning process gets stuck in a minimum, and it's more efficient to restart the learning than to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10, 10, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 1, 64)    1344        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 10, 64)    1344        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 5, 5, 64)     1216        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 640)          0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 640)          0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1600)         0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2880)         0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           184384      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 188,353\n",
      "Trainable params: 188,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 297us/step - loss: 0.6943 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 267us/step - loss: 0.6902 - acc: 0.5335 - val_loss: 0.6924 - val_acc: 0.5202\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 269us/step - loss: 0.6852 - acc: 0.5586 - val_loss: 0.6871 - val_acc: 0.5503\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 267us/step - loss: 0.6724 - acc: 0.6021 - val_loss: 0.6682 - val_acc: 0.6095\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 267us/step - loss: 0.6321 - acc: 0.6704 - val_loss: 0.6077 - val_acc: 0.6896\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 267us/step - loss: 0.5434 - acc: 0.7434 - val_loss: 0.5017 - val_acc: 0.7632\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 268us/step - loss: 0.4488 - acc: 0.7923 - val_loss: 0.3912 - val_acc: 0.8376\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 269us/step - loss: 0.3726 - acc: 0.8392 - val_loss: 0.3071 - val_acc: 0.8858\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10, 10, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 1, 64)    1344        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 10, 64)    1344        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 5, 5, 64)     1216        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 640)          0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 640)          0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1600)         0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2880)         0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           184384      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 188,353\n",
      "Trainable params: 188,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 10s 316us/step - loss: 0.2893 - acc: 0.8803 - val_loss: 0.2408 - val_acc: 0.9059\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 281us/step - loss: 0.2046 - acc: 0.9202 - val_loss: 0.1992 - val_acc: 0.9223\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.1724 - acc: 0.9348 - val_loss: 0.1795 - val_acc: 0.9359\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 279us/step - loss: 0.1608 - acc: 0.9385 - val_loss: 0.1688 - val_acc: 0.9410\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 278us/step - loss: 0.1411 - acc: 0.9459 - val_loss: 0.1572 - val_acc: 0.9389\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 278us/step - loss: 0.1344 - acc: 0.9477 - val_loss: 0.1543 - val_acc: 0.9396\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.1225 - acc: 0.9527 - val_loss: 0.1498 - val_acc: 0.9474\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/32000 [==============================] - 9s 282us/step - loss: 0.1152 - acc: 0.9561 - val_loss: 0.1413 - val_acc: 0.9477\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.1011 - acc: 0.9626 - val_loss: 0.1602 - val_acc: 0.9351\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 282us/step - loss: 0.0909 - acc: 0.9667 - val_loss: 0.1682 - val_acc: 0.9326\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.0828 - acc: 0.9682 - val_loss: 0.2327 - val_acc: 0.9331\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 278us/step - loss: 0.0746 - acc: 0.9721 - val_loss: 0.2921 - val_acc: 0.8828\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 279us/step - loss: 0.0659 - acc: 0.9741 - val_loss: 0.1661 - val_acc: 0.9390\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 278us/step - loss: 0.0598 - acc: 0.9777 - val_loss: 0.2975 - val_acc: 0.9293\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.0626 - acc: 0.9769 - val_loss: 0.2323 - val_acc: 0.9349\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 278us/step - loss: 0.0514 - acc: 0.9803 - val_loss: 0.2189 - val_acc: 0.9426\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 282us/step - loss: 0.0498 - acc: 0.9817 - val_loss: 0.2418 - val_acc: 0.9409\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 9s 277us/step - loss: 0.0470 - acc: 0.9822 - val_loss: 0.2866 - val_acc: 0.9344\n",
      "lr started with 0.0005343750000000002\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 10s 303us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.1521 - val_acc: 0.9539\n",
      "Learning FINISHED, final acc_test  0.953875  maximum one  0.953875\n"
     ]
    }
   ],
   "source": [
    "name_file_intantaneous=\"acceptancy_L\"+str(Lx)+\"_with_NS\"+str(NS)+\".txt\"\n",
    "file_out=open(name_file_intantaneous,\"w\")\n",
    "file_out.write(\"# it Acc_train Acc_test\"+\"\\n\")\n",
    "file_out.flush()\n",
    "\n",
    "Acc=[]\n",
    "Acc_val=[]\n",
    "\n",
    "\n",
    "\n",
    "restart=False\n",
    "\n",
    "acc=0\n",
    "it=0\n",
    "\n",
    "\n",
    "\n",
    "# we start with SGD optimizer until acc \n",
    "DCNN.compile(optimizer=opt_sgd,loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "DCNN.summary()\n",
    "while acc<0.995:\n",
    "    \n",
    "    hist=DCNN.fit(X_train.reshape(Strain,data_dim_x,data_dim_y,2),\n",
    "                     y_train,epochs=1,\n",
    "                         batch_size=b_size,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(X_test.reshape(Stest,data_dim_x,data_dim_y,2),y_test))\n",
    "\n",
    "    acc=hist.history['acc'][-1]\n",
    "    acc_val=hist.history['val_acc'][-1]\n",
    "    Acc.extend(hist.history['acc'])\n",
    "    Acc_val.extend(hist.history['val_acc'])\n",
    "    file_out.write(str(len(Acc))+\" \"+str(acc)+\" \"+str(acc_val)+\"\\n\")\n",
    "    file_out.flush()\n",
    "\n",
    "    if it>0 and len(Acc)>5 and abs((Acc[-1]-Acc[-2])/Acc[-1])<0.01: # If accuracy gets stuck, we decrease the learning rate\n",
    "        lr = K.get_value(DCNN.optimizer.lr)\n",
    "        K.set_value(DCNN.optimizer.lr, lr*.9)\n",
    "        print(\"lr changed to {}\".format(lr*.9))\n",
    "        it=0\n",
    "        if lr < 1e-5:\n",
    "            break\n",
    "    if it>2 and acc>0.8:\n",
    "        break\n",
    "\n",
    "    it+=1\n",
    "    \n",
    "    \n",
    "lr0 = K.get_value(DCNN.optimizer.lr) # we save the learning rate\n",
    "\n",
    "if acc<0.995:\n",
    "    #we change the optimizer to adam\n",
    "    DCNN.compile(optimizer=opt_adam,loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    DCNN.summary()\n",
    "\n",
    "\n",
    "it=0\n",
    "while acc<0.995:\n",
    "    \n",
    "    hist=DCNN.fit(X_train.reshape(Strain,data_dim_x,data_dim_y,2),\n",
    "             y_train,epochs=1,\n",
    "             batch_size=b_size,\n",
    "             shuffle=True,\n",
    "             validation_data=(X_test.reshape(Stest,data_dim_x,data_dim_y,2),y_test))\n",
    "    acc=hist.history['acc'][-1]\n",
    "    acc_val=hist.history['val_acc'][-1]\n",
    "    Acc.extend(hist.history['acc'])\n",
    "    Acc_val.extend(hist.history['val_acc'])\n",
    "    file_out.write(str(len(Acc))+\" \"+str(acc)+\" \"+str(acc_val)+\"\\n\")\n",
    "    file_out.flush()\n",
    "\n",
    "    if it>2 and abs(Acc[-1]-Acc[-2])<0.001 and Acc[-2]<Acc[-1]:# if the accuracy gets stuck we get out\n",
    "        break\n",
    "\n",
    "    \n",
    "    \n",
    "    it+=1\n",
    "\n",
    "    \n",
    "    \n",
    "while acc<0.995:\n",
    "\n",
    "    lr0=(1-acc)*0.03\n",
    "    print(\"lr started with {}\".format(lr0))\n",
    "    \n",
    "    #we come back to the SGD optimizer\n",
    "    DCNN.compile(optimizer=opt_sgd,loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    K.set_value(DCNN.optimizer.lr, lr0)\n",
    "    it=0\n",
    "    while acc<0.995:   \n",
    "        hist=DCNN.fit(X_train.reshape(Strain,data_dim_x,data_dim_y,2),\n",
    "                         y_train,epochs=1,\n",
    "                             batch_size=b_size,\n",
    "                             shuffle=True,\n",
    "                             validation_data=(X_test.reshape(Stest,data_dim_x,data_dim_y,2),y_test))\n",
    "\n",
    "        acc=hist.history['acc'][-1]\n",
    "        acc_val=hist.history['val_acc'][-1]\n",
    "        Acc.extend(hist.history['acc'])\n",
    "        Acc_val.extend(hist.history['val_acc'])\n",
    "        file_out.write(str(len(Acc))+\" \"+str(acc)+\" \"+str(acc_val)+\"\\n\")\n",
    "        file_out.flush()\n",
    "\n",
    "        if it>3 and len(Acc)>5 and abs((Acc[-1]-Acc[-2]))<0.005:\n",
    "            lr = K.get_value(DCNN.optimizer.lr)\n",
    "            K.set_value(DCNN.optimizer.lr, lr*.9)\n",
    "            print(\"lr changed to {}\".format(lr*.9))\n",
    "\n",
    "            it=0\n",
    "            if lr < 1e-3:\n",
    "                break\n",
    "\n",
    "        if it>1 and abs(Acc[-1]-Acc[-2])<0.0001  and Acc[-2]<Acc[-1]:\n",
    "            break\n",
    "\n",
    "        it+=1\n",
    "        \n",
    "    while acc<0.995:\n",
    "        #back to Adam optimizer\n",
    "        DCNN.compile(optimizer=opt_adam,loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "        DCNN.summary()\n",
    "\n",
    "        it=0\n",
    "        hist=DCNN.fit(X_train.reshape(Strain,data_dim_x,data_dim_y,2),\n",
    "                 y_train,epochs=1,\n",
    "                 batch_size=b_size,\n",
    "                 shuffle=True,\n",
    "                 validation_data=(X_test.reshape(Stest,data_dim_x,data_dim_y,2),y_test))\n",
    "        acc=hist.history['acc'][-1]\n",
    "        acc_val=hist.history['val_acc'][-1]\n",
    "        Acc.extend(hist.history['acc'])\n",
    "        Acc_val.extend(hist.history['val_acc'])\n",
    "        file_out.write(str(len(Acc))+\" \"+str(acc)+\" \"+str(acc_val)+\"\\n\")\n",
    "        file_out.flush()\n",
    "\n",
    "        \n",
    "        if it>2 and abs(Acc[-1]-Acc[-2])<0.01 :\n",
    "            break\n",
    "        it+=1\n",
    "\n",
    "        if acc<0.6:\n",
    "            restart=True\n",
    "            break\n",
    "\n",
    "    if len(Acc)>100:\n",
    "        if acc<0.99:\n",
    "            restart=True\n",
    "        break\n",
    "\n",
    "if restart:\n",
    "    print(\"Re-run the program (definition of the DCNN and fit), it got stuck!\")\n",
    "else:\n",
    "    print(\"Learning FINISHED, final acc_test \",acc_val,\" maximum one \",max(Acc_val))\n",
    "file_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
